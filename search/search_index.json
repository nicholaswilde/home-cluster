{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"My home Kubernetes cluster \u00b6 ... managed by Flux and serviced with RenovateBot My home Kubernetes (k3s) cluster managed by GitOps (Flux2) Background \u00b6 My cluster computing journey started last year when Jeff Geerling released his Turing Pi Cluster YouTube series . Now, I've always been into the Raspberry Pi ever since the release of version 1 and playing around with home networking projects such as running a Pi Hole server or a CUPS printer server, etc. The Turing Pi is a great \"cheap\" and compact way for my to branch out into cluster computing since I was tired of trying to remember which application I was running on which Raspberry Pi. After getting the Turing Pi 1, I noticed that there weren't very many charts available period, let alone for armv7 architectures. That led me into building my own Docker images and creating my Helm Charts repo which then led me to collaborating with the awesome k8s@home team . Now, here I am, rebuilding my cluster and diving into the GitOps world . Thanks \u00b6 A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes . License \u00b6 Apache 2.0 License Author \u00b6 This project was started in 2021 by Nicholas Wilde .","title":"Home"},{"location":"#my-home-kubernetes-cluster","text":"... managed by Flux and serviced with RenovateBot My home Kubernetes (k3s) cluster managed by GitOps (Flux2)","title":"My home Kubernetes cluster"},{"location":"#background","text":"My cluster computing journey started last year when Jeff Geerling released his Turing Pi Cluster YouTube series . Now, I've always been into the Raspberry Pi ever since the release of version 1 and playing around with home networking projects such as running a Pi Hole server or a CUPS printer server, etc. The Turing Pi is a great \"cheap\" and compact way for my to branch out into cluster computing since I was tired of trying to remember which application I was running on which Raspberry Pi. After getting the Turing Pi 1, I noticed that there weren't very many charts available period, let alone for armv7 architectures. That led me into building my own Docker images and creating my Helm Charts repo which then led me to collaborating with the awesome k8s@home team . Now, here I am, rebuilding my cluster and diving into the GitOps world .","title":"&nbsp; Background"},{"location":"#thanks","text":"A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes .","title":"&nbsp; Thanks"},{"location":"#license","text":"Apache 2.0 License","title":"&nbsp; License"},{"location":"#author","text":"This project was started in 2021 by Nicholas Wilde .","title":"&nbsp; Author"},{"location":"ansible/","text":"Ansible \u00b6 I use Ansible to setup the nodes and deploy K3s to my cluster. Inventory \u00b6 The inventory hosts and variables need to be updated to your environment. # ./server/ansible/inventory/cluster/hosts.yaml --- all : children : main : hosts : turing-main : ansible_host : 192.168.1.201 node : hosts : worker-02 : ansible_host : 192.168.1.202 worker-03 : ansible_host : 192.168.1.203 ... Be sure to set the ansible version and the user name on the nodes to the group_vars . # ./server/ansible/inventory/cluster/group_vars/all.yaml ... k3s_version : v1.18.16+k3s1 ansible_user : pi ... Playbooks \u00b6 All commands below are executed from the ./server/ansible directory except for the Task commands. Prepare \u00b6 Prepare the nodes for the K3s installation by changing certain settings and removing crufty packages. ansible-playbook ./playbook/prepare.yaml Install \u00b6 Install K3s ansible-playbook ./playbook/install.yaml Note This only works with Raspberry Pi OS 64-bit. Nuke \u00b6 Uninstall K3s from the cluster. ansible-playbook ./playbook/nuke.yaml Upgrade \u00b6 Perform an upgrade to the operatring system, not K3s. ansible-playbook ./playbook/upgrade.yaml Other Commands \u00b6 Other Ansible related commands can be found by running the following from the root of the repository: task -l","title":"Ansible"},{"location":"ansible/#ansible","text":"I use Ansible to setup the nodes and deploy K3s to my cluster.","title":"Ansible"},{"location":"ansible/#inventory","text":"The inventory hosts and variables need to be updated to your environment. # ./server/ansible/inventory/cluster/hosts.yaml --- all : children : main : hosts : turing-main : ansible_host : 192.168.1.201 node : hosts : worker-02 : ansible_host : 192.168.1.202 worker-03 : ansible_host : 192.168.1.203 ... Be sure to set the ansible version and the user name on the nodes to the group_vars . # ./server/ansible/inventory/cluster/group_vars/all.yaml ... k3s_version : v1.18.16+k3s1 ansible_user : pi ...","title":"Inventory"},{"location":"ansible/#playbooks","text":"All commands below are executed from the ./server/ansible directory except for the Task commands.","title":"Playbooks"},{"location":"ansible/#prepare","text":"Prepare the nodes for the K3s installation by changing certain settings and removing crufty packages. ansible-playbook ./playbook/prepare.yaml","title":"Prepare"},{"location":"ansible/#install","text":"Install K3s ansible-playbook ./playbook/install.yaml Note This only works with Raspberry Pi OS 64-bit.","title":"Install"},{"location":"ansible/#nuke","text":"Uninstall K3s from the cluster. ansible-playbook ./playbook/nuke.yaml","title":"Nuke"},{"location":"ansible/#upgrade","text":"Perform an upgrade to the operatring system, not K3s. ansible-playbook ./playbook/upgrade.yaml","title":"Upgrade"},{"location":"ansible/#other-commands","text":"Other Ansible related commands can be found by running the following from the root of the repository: task -l","title":"Other Commands"},{"location":"flux/","text":"Flux \u00b6 Installation \u00b6 brew install fluxcd/tap/flux Getting Started \u00b6 Bash completion # ~/.bashrc or ~/.bash_profile . < ( flux completion bash ) Check requirements flux check --pre \u25ba checking prerequisites \u2714 kubectl 1 .18.3 > = 1 .18.0 \u2714 kubernetes 1 .18.2 > = 1 .16.0 \u2714 prerequisites checks passed Run the bootstrap command to connect the cluster to a repo. Note Ensure that you have the GITHUB_USER environmental variable set and that you are already logged into the git repository. flux bootstrap github \\ --owner = $GITHUB_USER \\ --repository = home-cluster \\ --branch = main \\ --path = ./cluster \\ --personal Cluster Directory Layout \u00b6 ./cluster \u251c\u2500\u2500 flux-system \u2502 \u251c\u2500\u2500 gotk-components.yaml \u2502 \u251c\u2500\u2500 gotk-sync.yaml \u2502 \u251c\u2500\u2500 gotk-patches.yaml \u2502 \u251c\u2500\u2500 helm \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 namespace1 \u2502 \u251c\u2500\u2500 helm-release.yaml \u2502 \u2514\u2500\u2500 namespace1.yaml \u251c\u2500\u2500 namespace2 \u2502 \u251c\u2500\u2500 namespace2.yaml \u2502 \u251c\u2500\u2500 chart2 \u2502 \u2502 \u2514\u2500\u2500 helm-release.yaml \u2502 \u2514\u2500\u2500 chart3 \u2502 \u2514\u2500\u2500 helm-release.yaml \u2514\u2500\u2500 sources \u2514\u2500\u2500 some-repo-charts.yaml Hint The above chart files use the helm-release.yaml file name so that Renovate can correctly find the chart version. Namespaces \u00b6 Each cluster namespace gets its own directory in the cluster directory and the namespace is creates via a namespace.yaml manifest located in the namespace directory. --- apiVersion : v1 kind : Namespace metadata : name : <namespace name> Create a namespace using Task \u00b6 A namespace can also be created using task . The task will create a directory if it doesn't already exist as well as the namespace.yaml manifest. From the repo root directory, run the following: task ns:create NAME = <namespace name> Helm Charts \u00b6 Helm charts live inside the namespace directory. Whether or not the charts get their own directory depends on if there are multiple charts inside that namespace. HelmRelease Example \u00b6 Source \u00b6 The source file is typically added to the ./cluster/sources directory. flux create source helm repo-name-charts \\ --url https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ \\ --export | tee repo-name-charts.yaml All below commands are performed inside the ./cluster/<namespace name> directory. values.yaml \u00b6 Chart Specific \u00b6 --- nfs : path : /home/pi/nas/nfs server : 192.168.1.192 storageClass : defaultClass : false name : managed-nfs-storage Resources \u00b6 resources : limits : cpu : '200m' memory : '256Mi' requests : cpu : '100m' memory : '128Mi' HelmRelease \u00b6 The latter part of source parameter needs to match the name of the helm source . flux create helmrelease \\ chart-name \\ --source HelmRepository/repo-name-charts \\ --values values.yaml \\ --chart chart-name \\ --chart-version 1 .0.0 \\ --target-namespace namespace-name \\ --export \\ | tee helm-release.yaml Commit the yaml files and watch your cluster update. flux check watch flux get kustomizations watch flux get helmreleases watch flux get sources all -A Task \u00b6 WIP","title":"Flux"},{"location":"flux/#flux","text":"","title":"Flux"},{"location":"flux/#installation","text":"brew install fluxcd/tap/flux","title":"&nbsp; Installation"},{"location":"flux/#getting-started","text":"Bash completion # ~/.bashrc or ~/.bash_profile . < ( flux completion bash ) Check requirements flux check --pre \u25ba checking prerequisites \u2714 kubectl 1 .18.3 > = 1 .18.0 \u2714 kubernetes 1 .18.2 > = 1 .16.0 \u2714 prerequisites checks passed Run the bootstrap command to connect the cluster to a repo. Note Ensure that you have the GITHUB_USER environmental variable set and that you are already logged into the git repository. flux bootstrap github \\ --owner = $GITHUB_USER \\ --repository = home-cluster \\ --branch = main \\ --path = ./cluster \\ --personal","title":"&nbsp; Getting Started"},{"location":"flux/#cluster-directory-layout","text":"./cluster \u251c\u2500\u2500 flux-system \u2502 \u251c\u2500\u2500 gotk-components.yaml \u2502 \u251c\u2500\u2500 gotk-sync.yaml \u2502 \u251c\u2500\u2500 gotk-patches.yaml \u2502 \u251c\u2500\u2500 helm \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 namespace1 \u2502 \u251c\u2500\u2500 helm-release.yaml \u2502 \u2514\u2500\u2500 namespace1.yaml \u251c\u2500\u2500 namespace2 \u2502 \u251c\u2500\u2500 namespace2.yaml \u2502 \u251c\u2500\u2500 chart2 \u2502 \u2502 \u2514\u2500\u2500 helm-release.yaml \u2502 \u2514\u2500\u2500 chart3 \u2502 \u2514\u2500\u2500 helm-release.yaml \u2514\u2500\u2500 sources \u2514\u2500\u2500 some-repo-charts.yaml Hint The above chart files use the helm-release.yaml file name so that Renovate can correctly find the chart version.","title":"Cluster Directory Layout"},{"location":"flux/#namespaces","text":"Each cluster namespace gets its own directory in the cluster directory and the namespace is creates via a namespace.yaml manifest located in the namespace directory. --- apiVersion : v1 kind : Namespace metadata : name : <namespace name>","title":"Namespaces"},{"location":"flux/#create-a-namespace-using-task","text":"A namespace can also be created using task . The task will create a directory if it doesn't already exist as well as the namespace.yaml manifest. From the repo root directory, run the following: task ns:create NAME = <namespace name>","title":"Create a namespace using Task"},{"location":"flux/#helm-charts","text":"Helm charts live inside the namespace directory. Whether or not the charts get their own directory depends on if there are multiple charts inside that namespace.","title":"Helm Charts"},{"location":"flux/#helmrelease-example","text":"","title":"&nbsp; HelmRelease Example"},{"location":"flux/#source","text":"The source file is typically added to the ./cluster/sources directory. flux create source helm repo-name-charts \\ --url https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ \\ --export | tee repo-name-charts.yaml All below commands are performed inside the ./cluster/<namespace name> directory.","title":"Source"},{"location":"flux/#valuesyaml","text":"","title":"values.yaml"},{"location":"flux/#chart-specific","text":"--- nfs : path : /home/pi/nas/nfs server : 192.168.1.192 storageClass : defaultClass : false name : managed-nfs-storage","title":"Chart Specific"},{"location":"flux/#resources","text":"resources : limits : cpu : '200m' memory : '256Mi' requests : cpu : '100m' memory : '128Mi'","title":"Resources"},{"location":"flux/#helmrelease","text":"The latter part of source parameter needs to match the name of the helm source . flux create helmrelease \\ chart-name \\ --source HelmRepository/repo-name-charts \\ --values values.yaml \\ --chart chart-name \\ --chart-version 1 .0.0 \\ --target-namespace namespace-name \\ --export \\ | tee helm-release.yaml Commit the yaml files and watch your cluster update. flux check watch flux get kustomizations watch flux get helmreleases watch flux get sources all -A","title":"HelmRelease"},{"location":"flux/#task","text":"WIP","title":"Task"},{"location":"k3s/","text":"K3s \u00b6 My cluster runs the K3S version of kubernetes due to its lightness in weight. Deployment \u00b6 I'm currently using the k3s-ansible repo to deploy K3S on my cluster. My plan is eventually create my own ansible playbook and save it in this repo. After making a copy of the inventory directory, be sure to change the values in group_vars/all.yaml file. The easiest way to see the latest k3s_version is to browse the k3s upgrade channels . # group_vars/all.yml --- k3s_version : v1.17.5+k3s1 ansible_user : pi I also add the following extra_server_args to ensure that pods don't bog down my master. # group_vars/all.yml --- extra_server_args : \"--node-taint CriticalAddonsOnly=true:NoExecute\" Update the hosts.ini file with the ip addresses of your nodes. Deploy k3s: ansible-playbook site.yml Reset \u00b6 The cluster may be reset by running the reset.yml file. ansible-playbook reset.yml","title":"K3s"},{"location":"k3s/#k3s","text":"My cluster runs the K3S version of kubernetes due to its lightness in weight.","title":"K3s"},{"location":"k3s/#deployment","text":"I'm currently using the k3s-ansible repo to deploy K3S on my cluster. My plan is eventually create my own ansible playbook and save it in this repo. After making a copy of the inventory directory, be sure to change the values in group_vars/all.yaml file. The easiest way to see the latest k3s_version is to browse the k3s upgrade channels . # group_vars/all.yml --- k3s_version : v1.17.5+k3s1 ansible_user : pi I also add the following extra_server_args to ensure that pods don't bog down my master. # group_vars/all.yml --- extra_server_args : \"--node-taint CriticalAddonsOnly=true:NoExecute\" Update the hosts.ini file with the ip addresses of your nodes. Deploy k3s: ansible-playbook site.yml","title":"&nbsp; Deployment"},{"location":"k3s/#reset","text":"The cluster may be reset by running the reset.yml file. ansible-playbook reset.yml","title":"&nbsp; Reset"},{"location":"operating-system/","text":"Operating System \u00b6 My cluster uses a modified version of Raspberry Pi OS 64-bit because of its native support for the CM3+ and it's lightness in weight. I decided to use the 64-bit version of the OS on my cluster because there are not that many charts that notively support the armv7 architecture and I got tired of having to build my own images and charts. The problem is that the RPiOS is only released in a full desktop version and so I had to uninstall all of the X11 related components in order to free up some disk space on the CM3+. Download \u00b6 Download the latest release of the RPiOS 64-bit . I've recently started building my own RPiOS 64-bit lite version since the official version has been stuck in beta for so long and it is only offered in the desktop/full version. You may download the latest release by using the following: wget https://github.com/nicholaswilde/pi-gen/releases/latest/download/raspios-buster-arm64-lite.zip Alternatively, you can download the 32-bit Lite version that doesn't need any modifications. Ubuntu arm64 was a bit heavy for my CM3+. Installation \u00b6 Installing the operating system on the CM3+ was done using the Turing Pi using these instructions from Turing Pi using usbboot . Alternatively, a compute module I/O board can be used to flash the CM3+. Modification \u00b6 Note This step may be skipped if you are already using the lite version. To rip out all of the desktop related apps in the full version, simply remove any modules that are related to X11. ( sudo apt --purge remove -y \"x11-* lxsession\" sudo apt --purge -y autoremove sudo apt update sudo apt upgrade -y ) I didn't spend too much time stripping down the OS to the bare essentials because I wanted to get my cluster up and running. You may reverse engineer this guide to further clean up the OS. Suggestions are always welcome! You can also go through the stage3, 4, & 5 folders in the official pi-gen repo and remove all of those packages as well. Reboot if needed. sudo reboot Other Settings \u00b6 Hostname \u00b6 The hostname can be changed with the following: sudo hostnamectl set-hostname <hostname> SSH Key \u00b6 Import your public SSH key so that you don't need to enter the dang password anymore when logging into each node. Note Your public key needs to exist on GiHub at https://github.com/nicholaswilde.keys ssh-import-id-gh <username> Docker \u00b6 Last thing I need to do is install Docker on the master and each node. ( set -x ; sudo apt-get update && sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release && DIST = $( lsb_release -is | tr '[:upper:]' '[:lower:]' ) && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(armhf\\)\\(64\\)\\?.*/\\1\\2hf/' -e 's/aarch64$/arm64/' ) \" && curl -fsSL \"https://download.docker.com/linux/ ${ DIST } /gpg\" | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg && echo \"deb [arch= ${ ARCH } signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ ${ DIST } $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update && sudo apt install -y docker-ce docker-ce-cli containerd.io && sudo usermod -aG docker $( whoami ) ) Be sure to reboot. sudo reboot Test to make sure that Docker can run without using sudo . docker run --rm hello-world","title":"Operating System"},{"location":"operating-system/#operating-system","text":"My cluster uses a modified version of Raspberry Pi OS 64-bit because of its native support for the CM3+ and it's lightness in weight. I decided to use the 64-bit version of the OS on my cluster because there are not that many charts that notively support the armv7 architecture and I got tired of having to build my own images and charts. The problem is that the RPiOS is only released in a full desktop version and so I had to uninstall all of the X11 related components in order to free up some disk space on the CM3+.","title":"Operating System"},{"location":"operating-system/#download","text":"Download the latest release of the RPiOS 64-bit . I've recently started building my own RPiOS 64-bit lite version since the official version has been stuck in beta for so long and it is only offered in the desktop/full version. You may download the latest release by using the following: wget https://github.com/nicholaswilde/pi-gen/releases/latest/download/raspios-buster-arm64-lite.zip Alternatively, you can download the 32-bit Lite version that doesn't need any modifications. Ubuntu arm64 was a bit heavy for my CM3+.","title":"&nbsp;  Download"},{"location":"operating-system/#installation","text":"Installing the operating system on the CM3+ was done using the Turing Pi using these instructions from Turing Pi using usbboot . Alternatively, a compute module I/O board can be used to flash the CM3+.","title":"&nbsp; Installation"},{"location":"operating-system/#modification","text":"Note This step may be skipped if you are already using the lite version. To rip out all of the desktop related apps in the full version, simply remove any modules that are related to X11. ( sudo apt --purge remove -y \"x11-* lxsession\" sudo apt --purge -y autoremove sudo apt update sudo apt upgrade -y ) I didn't spend too much time stripping down the OS to the bare essentials because I wanted to get my cluster up and running. You may reverse engineer this guide to further clean up the OS. Suggestions are always welcome! You can also go through the stage3, 4, & 5 folders in the official pi-gen repo and remove all of those packages as well. Reboot if needed. sudo reboot","title":"&nbsp; Modification"},{"location":"operating-system/#other-settings","text":"","title":"&nbsp; Other Settings"},{"location":"operating-system/#hostname","text":"The hostname can be changed with the following: sudo hostnamectl set-hostname <hostname>","title":"&nbsp; Hostname"},{"location":"operating-system/#ssh-key","text":"Import your public SSH key so that you don't need to enter the dang password anymore when logging into each node. Note Your public key needs to exist on GiHub at https://github.com/nicholaswilde.keys ssh-import-id-gh <username>","title":"&nbsp; SSH Key"},{"location":"operating-system/#docker","text":"Last thing I need to do is install Docker on the master and each node. ( set -x ; sudo apt-get update && sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release && DIST = $( lsb_release -is | tr '[:upper:]' '[:lower:]' ) && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(armhf\\)\\(64\\)\\?.*/\\1\\2hf/' -e 's/aarch64$/arm64/' ) \" && curl -fsSL \"https://download.docker.com/linux/ ${ DIST } /gpg\" | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg && echo \"deb [arch= ${ ARCH } signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ ${ DIST } $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update && sudo apt install -y docker-ce docker-ce-cli containerd.io && sudo usermod -aG docker $( whoami ) ) Be sure to reboot. sudo reboot Test to make sure that Docker can run without using sudo . docker run --rm hello-world","title":"&nbsp; Docker"},{"location":"restoration/","text":"Restoration \u00b6 How to restore a cluster using flux after installing an operating system on all cluster nodes. The first step is to get a base K3s installation installed on your cluster. This can be done be using the Ansible Install playbook. TL;DR \u00b6 task k3s:bootstrap flux-system Namespace \u00b6 The next thing to do is create a flux-system namespace to install the sops-gpg secret so that flux can properly unencrypt any secrets in the repository. kubectl create namespace flux-system Mozilla SOPS \u00b6 Once the flux-system namespace is created, import the private gpg key back into the cluster using whichever method you prefer. Restore Private GPG Key using pass \u00b6 pass gpg/home-cluster-private | kubectl create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin Note Where gpg/home-cluster-private is the location of the home-cluster private key in pass . Restore Private GPG Key from the gpg keyring \u00b6 gpg --export-secret-keys --armor \" ${ KEY_FP } \" | kubectl create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin Note Where \"${KEY_FP}\" is the fingerprint of the home-cluster gpg key. Restore Using the Flux CLI \u00b6 Run the bootstrap command: flux bootstrap github \\ --owner = $GITHUB_USER \\ --repository = home-cluster \\ --branch = main \\ --path = ./cluster \\ --personal The above command will clone the repository, update the components manifest in /flux-system/gotk-components.yaml and it will push the changes to the remote branch. Tell Flux to pull the manifests from Git and upgrade itself with: flux reconcile source git flux-system Verify that the controllers have been upgrade with: flux check Note If you are having trouble restoring you cluster, try deleting the deploy key from your cluster repo. Pull the Repo \u00b6 After flux syncs with the repo, you'll need to perform a git pull in order to update your local repo. git pull origin main","title":"Restoration"},{"location":"restoration/#restoration","text":"How to restore a cluster using flux after installing an operating system on all cluster nodes. The first step is to get a base K3s installation installed on your cluster. This can be done be using the Ansible Install playbook.","title":"Restoration"},{"location":"restoration/#tldr","text":"task k3s:bootstrap","title":"TL;DR"},{"location":"restoration/#flux-system-namespace","text":"The next thing to do is create a flux-system namespace to install the sops-gpg secret so that flux can properly unencrypt any secrets in the repository. kubectl create namespace flux-system","title":"flux-system Namespace"},{"location":"restoration/#mozilla-sops","text":"Once the flux-system namespace is created, import the private gpg key back into the cluster using whichever method you prefer.","title":"Mozilla SOPS"},{"location":"restoration/#restore-private-gpg-key-using-pass","text":"pass gpg/home-cluster-private | kubectl create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin Note Where gpg/home-cluster-private is the location of the home-cluster private key in pass .","title":"Restore Private GPG Key using pass"},{"location":"restoration/#restore-private-gpg-key-from-the-gpg-keyring","text":"gpg --export-secret-keys --armor \" ${ KEY_FP } \" | kubectl create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin Note Where \"${KEY_FP}\" is the fingerprint of the home-cluster gpg key.","title":"Restore Private GPG Key from the gpg keyring"},{"location":"restoration/#restore-using-the-flux-cli","text":"Run the bootstrap command: flux bootstrap github \\ --owner = $GITHUB_USER \\ --repository = home-cluster \\ --branch = main \\ --path = ./cluster \\ --personal The above command will clone the repository, update the components manifest in /flux-system/gotk-components.yaml and it will push the changes to the remote branch. Tell Flux to pull the manifests from Git and upgrade itself with: flux reconcile source git flux-system Verify that the controllers have been upgrade with: flux check Note If you are having trouble restoring you cluster, try deleting the deploy key from your cluster repo.","title":"Restore Using the Flux CLI"},{"location":"restoration/#pull-the-repo","text":"After flux syncs with the repo, you'll need to perform a git pull in order to update your local repo. git pull origin main","title":"Pull the Repo"},{"location":"sops/","text":"Mozilla SOPS \u00b6 I'm currently using Mozilla SOPS to manage secrets on my cluster. Deployment \u00b6 I used the Flux Mozilla SOPS Guide to help setup SOPS on my cluster excluding the Configure in-cluster secrets decryption section. The biggest problem I had was understanding the asymmetric encryption mechanism that SOPS uses. I found that the Manage Kubernetes Secrets with Mozilla SOPS & Flux 2 (with Leigh Capili) video on YouTube helped understand the concept a lot. I also didn't spend the time to figure out how to patch the ./flux-system/gotk-sync.yaml with a separate file and so I just ended up editing the file directly. # ./flux-system/gotk-sync.yaml ... spec : decryption : provider : sops secretRef : name : sops-gpg ... Make sure that the flux-system Kustomization file updates properly after applying the patch. kubectl get Kustomization -n flux-system Make sure that the basic-auth was created successfully. kubectl describe Kustomization flux-system -n flux-system | grep basic-auth secret/basic-auth created Commands \u00b6 Here are some commands that I found helpful when setting up SOPS . Encrypt the basic-auth.yaml secret with SOPS using your GPG key: sops --encrypt --in-place basic-auth.yaml Output decoded secrets without external tools. Used when testing the basic-auth example. kubectl get secret my-secret -o go-template = '{{range $k,$v := .data}}{{\"### \"}}{{$k}}{{\"\\n\"}}{{$v|base64decode}}{{\"\\n\\n\"}}{{end}}' Import the Public GPG Key Using Task \u00b6 Task may be used to import the public key. task gpg:import Check for Unencrypted Secrets Using pre-commit \u00b6 I use a custom pre-commit hook that runs this script to check that resources of kind secret are encrypted using SOPS . pre-commit run check-sops-secrets Note The scipt does not check if the secret is out of date!","title":"Mozilla SOPS"},{"location":"sops/#mozilla-sops","text":"I'm currently using Mozilla SOPS to manage secrets on my cluster.","title":"Mozilla SOPS"},{"location":"sops/#deployment","text":"I used the Flux Mozilla SOPS Guide to help setup SOPS on my cluster excluding the Configure in-cluster secrets decryption section. The biggest problem I had was understanding the asymmetric encryption mechanism that SOPS uses. I found that the Manage Kubernetes Secrets with Mozilla SOPS & Flux 2 (with Leigh Capili) video on YouTube helped understand the concept a lot. I also didn't spend the time to figure out how to patch the ./flux-system/gotk-sync.yaml with a separate file and so I just ended up editing the file directly. # ./flux-system/gotk-sync.yaml ... spec : decryption : provider : sops secretRef : name : sops-gpg ... Make sure that the flux-system Kustomization file updates properly after applying the patch. kubectl get Kustomization -n flux-system Make sure that the basic-auth was created successfully. kubectl describe Kustomization flux-system -n flux-system | grep basic-auth secret/basic-auth created","title":"&nbsp; Deployment"},{"location":"sops/#commands","text":"Here are some commands that I found helpful when setting up SOPS . Encrypt the basic-auth.yaml secret with SOPS using your GPG key: sops --encrypt --in-place basic-auth.yaml Output decoded secrets without external tools. Used when testing the basic-auth example. kubectl get secret my-secret -o go-template = '{{range $k,$v := .data}}{{\"### \"}}{{$k}}{{\"\\n\"}}{{$v|base64decode}}{{\"\\n\\n\"}}{{end}}'","title":"&nbsp; Commands"},{"location":"sops/#import-the-public-gpg-key-using-task","text":"Task may be used to import the public key. task gpg:import","title":"&nbsp; Import the Public GPG Key Using Task"},{"location":"sops/#check-for-unencrypted-secrets-using-pre-commit","text":"I use a custom pre-commit hook that runs this script to check that resources of kind secret are encrypted using SOPS . pre-commit run check-sops-secrets Note The scipt does not check if the secret is out of date!","title":"Check for Unencrypted Secrets Using pre-commit"},{"location":"specs/","text":"Specs \u00b6 Specifications for my home cluster. Software \u00b6 Raspberry Pi OS 64-bit k3s Flux Mozilla SOPS Hardware \u00b6 Turing Pi - 1X Raspberry Pi CM3+ - 1X master - 6X nodes Raspberry Pi 4 B - 1X serving 5TB via NFS MITXPC MX500-USB3 Compact Mini-ITX Case - 1X Noctua NF-A4x10 Fans - 2X Storage \u00b6 I'm using using anything fancy for storage such as rook-ceph nor Longhorn . Just a 5TB disk being served by a Raspberry Pi 4 B via NFS and integrated into the cluster using nfs-subdir-external-provisioner . Networking \u00b6 I don't use any fancy networking in my cluster, yet. This is definitely a topic that I want tackle at a later date. Currently, I'm using Traefik with external-dns to serve my applications. Automation \u00b6 I try to use automaiton as much as possible when developing my projects so that I don't have to remember or perform the same tasks over and over. Some of those automations include using pre-commit , Task , and GitHub Actions. Tools \u00b6 Ansible Task pre-commit usbboot kubectl krew change-ns cert-manager Kubernetes NameSpace Killer (knsk)","title":"Specs"},{"location":"specs/#specs","text":"Specifications for my home cluster.","title":"Specs"},{"location":"specs/#software","text":"Raspberry Pi OS 64-bit k3s Flux Mozilla SOPS","title":"&nbsp; Software"},{"location":"specs/#hardware","text":"Turing Pi - 1X Raspberry Pi CM3+ - 1X master - 6X nodes Raspberry Pi 4 B - 1X serving 5TB via NFS MITXPC MX500-USB3 Compact Mini-ITX Case - 1X Noctua NF-A4x10 Fans - 2X","title":"&nbsp; Hardware"},{"location":"specs/#storage","text":"I'm using using anything fancy for storage such as rook-ceph nor Longhorn . Just a 5TB disk being served by a Raspberry Pi 4 B via NFS and integrated into the cluster using nfs-subdir-external-provisioner .","title":"&nbsp; Storage"},{"location":"specs/#networking","text":"I don't use any fancy networking in my cluster, yet. This is definitely a topic that I want tackle at a later date. Currently, I'm using Traefik with external-dns to serve my applications.","title":"&nbsp; Networking"},{"location":"specs/#automation","text":"I try to use automaiton as much as possible when developing my projects so that I don't have to remember or perform the same tasks over and over. Some of those automations include using pre-commit , Task , and GitHub Actions.","title":"&nbsp; Automation"},{"location":"specs/#tools","text":"Ansible Task pre-commit usbboot kubectl krew change-ns cert-manager Kubernetes NameSpace Killer (knsk)","title":"&nbsp; Tools"},{"location":"tls/","text":"TLS \u00b6 How to add TLS certificates to HTTPS ingress resources. Generate Certificate Files \u00b6 The following command will generate tls.crt and tls.key files that can then be imported into Kubernetes. Interactive openssl req \\ -newkey rsa:2048 -nodes -keyout tls.key \\ -x509 -days 365 -out tls.crt Non-interactive openssl req -new -newkey rsa:4096 -days 365 -nodes -x509 \\ -subj \"/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com\" \\ -keyout www.example.com.key -out www.example.com.cert Generate Secret Resource from Certificate \u00b6 You can then import the certifates into Kubernetes as a secret. kubectl create secret generic traefik-cert \\ --from-file = tls.crt \\ --from-file = tls.key Optionally, you can export the secret to a yaml file to save in the repo. kubectl create secret generic traefik-cert \\ --from-file = tls.crt \\ --from-file = tls.key \\ --dry-run = client -n kubernetes-dashboard -o yaml \\ | tee traefik-cert-secret.yaml You can also encrypt the secret using SOPS before pushing it to the repo. sops --encrypt --in-place traefik-cert-secret.yaml Optional \u00b6 You can optionally backup the certificate to a password manager such as pass . Backup Certificate to pass \u00b6 pass insert -fm tls/crt < tls.crt pass insert -fm tls/key < tls.key Note where tls/crt and tls/key are the pass entries. Generate the Secret Resource from pass \u00b6 You can also then create the secret resource file using the certificate from pass . kubectl create secret generic traefik-cert \\ --from-literal = tls.cert = \" $( pass tls/crt ) \" \\ --from-literal = tls.key = \" $( pass tls/key ) \" \\ --dry-run = client -n kubernetes-dashboard -o yaml \\ | tee traefik-cert-secret.yaml Configure Traefik \u00b6 You need to configure Traefik to reference the new secret. ConfigMap \u00b6 Edit the Traefik ConfigMap. kubectl edit configmap traefik -n kube-system Todo Patch the existing CM instead. ... data : traefik.toml : | defaultEntryPoints = [\"http\",\"https\"] [entryPoints] [entryPoints.http] address = \":80\" [entryPoints.http.redirect] entryPoint = \"https\" [entryPoints.https] address = \":443\" [entryPoints.https.tls] [[entryPoints.https.tls.certificates]] CertFile = \"/ssl/tls.crt\" KeyFile = \"/ssl/tls.key\" ... Deployment \u00b6 Edit the Traefik deployment to reference the new secret. kubectl edit deployment traefik -n kube-system Todo Patch the existing Deployment instead. ... spec : volumes : - name : ssl secret : secretName : traefik-cert ... Ingress \u00b6 Add the reference to the new secret to the ingress resource. ... spec : tls : - secretName : traefik-cert ... References \u00b6 Using Traefik with TLS on Kubernetes","title":"TLS"},{"location":"tls/#tls","text":"How to add TLS certificates to HTTPS ingress resources.","title":"TLS"},{"location":"tls/#generate-certificate-files","text":"The following command will generate tls.crt and tls.key files that can then be imported into Kubernetes. Interactive openssl req \\ -newkey rsa:2048 -nodes -keyout tls.key \\ -x509 -days 365 -out tls.crt Non-interactive openssl req -new -newkey rsa:4096 -days 365 -nodes -x509 \\ -subj \"/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com\" \\ -keyout www.example.com.key -out www.example.com.cert","title":"Generate Certificate Files"},{"location":"tls/#generate-secret-resource-from-certificate","text":"You can then import the certifates into Kubernetes as a secret. kubectl create secret generic traefik-cert \\ --from-file = tls.crt \\ --from-file = tls.key Optionally, you can export the secret to a yaml file to save in the repo. kubectl create secret generic traefik-cert \\ --from-file = tls.crt \\ --from-file = tls.key \\ --dry-run = client -n kubernetes-dashboard -o yaml \\ | tee traefik-cert-secret.yaml You can also encrypt the secret using SOPS before pushing it to the repo. sops --encrypt --in-place traefik-cert-secret.yaml","title":"Generate Secret Resource from Certificate"},{"location":"tls/#optional","text":"You can optionally backup the certificate to a password manager such as pass .","title":"Optional"},{"location":"tls/#backup-certificate-to-pass","text":"pass insert -fm tls/crt < tls.crt pass insert -fm tls/key < tls.key Note where tls/crt and tls/key are the pass entries.","title":"Backup Certificate to pass"},{"location":"tls/#generate-the-secret-resource-from-pass","text":"You can also then create the secret resource file using the certificate from pass . kubectl create secret generic traefik-cert \\ --from-literal = tls.cert = \" $( pass tls/crt ) \" \\ --from-literal = tls.key = \" $( pass tls/key ) \" \\ --dry-run = client -n kubernetes-dashboard -o yaml \\ | tee traefik-cert-secret.yaml","title":"Generate the Secret Resource from pass"},{"location":"tls/#configure-traefik","text":"You need to configure Traefik to reference the new secret.","title":"Configure Traefik"},{"location":"tls/#configmap","text":"Edit the Traefik ConfigMap. kubectl edit configmap traefik -n kube-system Todo Patch the existing CM instead. ... data : traefik.toml : | defaultEntryPoints = [\"http\",\"https\"] [entryPoints] [entryPoints.http] address = \":80\" [entryPoints.http.redirect] entryPoint = \"https\" [entryPoints.https] address = \":443\" [entryPoints.https.tls] [[entryPoints.https.tls.certificates]] CertFile = \"/ssl/tls.crt\" KeyFile = \"/ssl/tls.key\" ...","title":"ConfigMap"},{"location":"tls/#deployment","text":"Edit the Traefik deployment to reference the new secret. kubectl edit deployment traefik -n kube-system Todo Patch the existing Deployment instead. ... spec : volumes : - name : ssl secret : secretName : traefik-cert ...","title":"Deployment"},{"location":"tls/#ingress","text":"Add the reference to the new secret to the ingress resource. ... spec : tls : - secretName : traefik-cert ...","title":"Ingress"},{"location":"tls/#references","text":"Using Traefik with TLS on Kubernetes","title":"References"},{"location":"namespaces/dev/","text":"dev \u00b6 A namespace in which I developt new charts.","title":"dev"},{"location":"namespaces/dev/#dev","text":"A namespace in which I developt new charts.","title":"dev"},{"location":"namespaces/home/","text":"home \u00b6 WIP","title":"home"},{"location":"namespaces/home/#home","text":"WIP","title":"home"},{"location":"namespaces/media/","text":"media \u00b6 WIP","title":"media"},{"location":"namespaces/media/#media","text":"WIP","title":"media"},{"location":"namespaces/system-upgrade/","text":"system-upgrade \u00b6 The system-upgrade namespace just houses the k3s automated upgrade deployment which uses Rancher's system-upgrade-controller to leverage a custom resource definition (CRD) , the plan , and a controller to schedule upgrades based on the configured plans.","title":"system-upgrade"},{"location":"namespaces/system-upgrade/#system-upgrade","text":"The system-upgrade namespace just houses the k3s automated upgrade deployment which uses Rancher's system-upgrade-controller to leverage a custom resource definition (CRD) , the plan , and a controller to schedule upgrades based on the configured plans.","title":"system-upgrade"},{"location":"namespaces/test/","text":"test \u00b6 A namespace in which I test charts on my cluster.","title":"test"},{"location":"namespaces/test/#test","text":"A namespace in which I test charts on my cluster.","title":"test"}]}